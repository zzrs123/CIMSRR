# 强化学习-学习笔记10 | 经验回放

Experience Replay 经验回放。价值学习高级技巧第一篇。

之前讲解的 价值学习的方法 都很简单，所以实现效果并不优秀。接下来会介绍一些高级的技巧，可以大幅度提高 DQN 的表现。Experience Replay 是最重要的技巧。



## 10. 经验回放

### 10.1 DQN / Deep Q Network

DQN 是用神经网络 $Q(s,a;w)$ 来近似 Q-star 函数。通常用 TD 算法训练 DQN。TD算法思路如下：

- agent 观察当前状态 $s_t$ 并执行动作 $a_t$；

- 环境给出新的状态 $s_{t+1}$ ，返回奖励 $r_t$；

- TD target： $y_t = r_t + \gamma \cdot {\mathop{max}\limits_{a}Q^*(s_{t+1},a;w)}$；

- TD error：$\delta_t = q_t - y_t; \ \ 其中q_t=Q(s_t,a_t;w)$

  $q_t$ 完全是猜测的，$y_t$ 有一部分真实值。

- 我们的目标是让 $q_t$ 接近 $y_t$，所以最小化 $\delta^2$，即：

- $\delta_t$的均值记为：$L(w)=\frac{1}{T}\sum_{t=1}^{T}\frac{\delta^2_t}{2}$，通过调整 w ，使得 L(w) 尽量小。

- 之前是使用在线梯度下降来更新 w：

  - 得到 transition $(s_t,a_t,r_t,s_{t+1})$，计算 $\delta_t$
  - 计算梯度 $g_t = \frac{\partial \ \delta^2_t}{\partial \ w} = \delta_t \cdot \frac{\partial \ Q(s_t,a_t;w)}{\partial \ w}$
  - 梯度下降更新 w：$w \leftarrow w-\alpha\cdot g_t$
  - 接着是**之前忽略掉的一点**，在更新完 w 之后，**我们丢弃了这次使用的 transition。**



### 10.2 DQN的不足之处

#### a. 经验浪费

这里的经验（包括经验回放）是指 从开始到结束所有的 ***transitions***，刚才回顾的TD算法在使用了一次 transition 后就将它丢弃，这就造成了经验的浪费。



#### b. 相关更新

另外一个不足的原因是，前后两个 transition 之间存在很强的关联。实践证明这种相关性是 有害的。如果能够把 这些 transition 打散，更有利于训练 DQN。



经验回放就可以很好克服以上两个缺点。



### 10.3 经验回放原理

一个 transition 相当于一条训练数据，为了重复使用之前的 transition，可以把 最近的 transition 存入一个**队列(Replay Buffer)**，容量为 n，n 是一个超参数。

经验回放 的使用方法：

- 我们的目标是 将$L(w)=\frac{1}{T}\sum_{t=1}^{T}\frac{\delta^2_t}{2}$最小化

- 使用随机梯度下降改进 L(w)；

  - 每次从 Replay Buffer中随机均匀抽样一个 transition $(s_i,a_i,r_i,s_{i+1})$

    > 注意这里可以随机抽取多个 **transition**

  - 计算 TD error $\delta_i$。

  - 计算随机梯度：$g_i = \frac{\partial \ \delta^2_i/2}{\partial w}=\delta_i \cdot \frac{\partial \ Q(s_i,a_i;w)}{\partial \ w}$

  - 随机梯度下降：$w \leftarrow w-\alpha\cdot g_i$



经验回放的好处就是；

1. 打破 transition 之间的相关性；
2. 重复使用之前的经验。



### 10.4 经验回放改进

经验回放以改进效果明显而被广泛使用，也有了很多对经验回放的改进。下面介绍一种：优先经验回放 / Prioritized Experience Reply。

改进之处在于使用非均匀抽样来选择 transitions。



优先经验回放的想法如下：

- 队列里存了很多 transitions，但是它们的重要性并不一样；

- 我们可以通过 TD error 来判断 transitions 的重要性，绝对值越大，就说明越重要。

- 原因是，这里的优先级是指，我们要优先训练 难以训练的、训练样本少的、训练效果不好的这些场景，给与这些 transition 更大的重视（权重）

  > （就好像要多看看自己的错题，而不是自己很容易做对的题）

  而 TD error 大的 transition，实际意义是与实际值偏离大，说明对于场景不熟悉。



抽样方法介绍两种：

1. 使抽样概率与 TD-error 线性相关：$p_t\propto|\gamma_t|+\epsilon$

   $\epsilon$ 是一个很小的数，避免 $p_t$ 为 0 ；

2. 对$|\delta_t|$ 作排序，rank(t)是其序号，大的靠前，小的靠后：$p_t\propto \frac{1}{rank(t)}$，这样就让排在前面的更容易被抽到。



调整学习率矫正偏差：

因为不同的 transition 被人为附上了不同的概率，成为了非均匀抽样，这样会造成 DQN预测有偏差，通过调整学习率来矫正：

当所有 transition 的概率都一样，那么所有的 transition 的学习率一样。如果非均匀抽样，需要根据不同的 transition 概率来调整学习率。

- 如果 transition 的抽样概率较大，应当调小它的学习率；
- 具体操作是用$(n \cdot p_t)^{-\beta},\beta\in(0,1)$ 乘以 学习率 α，n 是队列容量，β 是一个0~1之间的数，是超参数，论文中一般从很小增长到1。



下面再考虑一个事情：

即，在优先经验回放中，我们需要把 TD-error 跟 transtions 放在一起，而刚放入队列的 transition **还没有进行训练** DQN，不知道 TD-error，这时直接把它的 $\delta$ 设为最大值，给他最高的权重；

每次我们使用了 transition 之后，重新计算 它的 $\delta_t$，更新 transition 的权重。




## x. 参考教程

- 视频课程：[深度强化学习（全）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1rv41167yx)
- 视频原地址：https://www.youtube.com/user/wsszju
- 课件地址：https://github.com/wangshusen/DeepLearning
- 经验回放论文：[Reinforcement Learning for Robots Using Neural Networks](http://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf)
- 优先经验回放论文：[Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)

